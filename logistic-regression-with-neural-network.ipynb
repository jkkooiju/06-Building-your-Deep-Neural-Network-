{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7105432,"sourceType":"datasetVersion","datasetId":4096301},{"sourceId":7105541,"sourceType":"datasetVersion","datasetId":4096386},{"sourceId":7105662,"sourceType":"datasetVersion","datasetId":4096478}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os as os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n#from lr_utils import load_dataset\n\n%matplotlib inline\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:25.133061Z","iopub.execute_input":"2023-12-03T00:20:25.133539Z","iopub.status.idle":"2023-12-03T00:20:26.222481Z","shell.execute_reply.started":"2023-12-03T00:20:25.133505Z","shell.execute_reply":"2023-12-03T00:20:26.221106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport h5py\n    \n    \ndef load_dataset():\n    train_dataset = h5py.File('/kaggle/input/catvnocat/10-train_catvnoncat.h5','r')\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('/kaggle/input/catvnocat/09-test_catvnoncat.h5',\"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.225392Z","iopub.execute_input":"2023-12-03T00:20:26.226322Z","iopub.status.idle":"2023-12-03T00:20:26.236535Z","shell.execute_reply.started":"2023-12-03T00:20:26.226244Z","shell.execute_reply":"2023-12-03T00:20:26.235360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.238342Z","iopub.execute_input":"2023-12-03T00:20:26.239072Z","iopub.status.idle":"2023-12-03T00:20:26.321081Z","shell.execute_reply.started":"2023-12-03T00:20:26.239026Z","shell.execute_reply":"2023-12-03T00:20:26.319424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.","metadata":{}},{"cell_type":"code","source":"# Example of a picture\nindex = 55\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.324593Z","iopub.execute_input":"2023-12-03T00:20:26.326326Z","iopub.status.idle":"2023-12-03T00:20:26.684562Z","shell.execute_reply.started":"2023-12-03T00:20:26.326254Z","shell.execute_reply":"2023-12-03T00:20:26.682905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of a picture\nindex = 25\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.686446Z","iopub.execute_input":"2023-12-03T00:20:26.686933Z","iopub.status.idle":"2023-12-03T00:20:26.984901Z","shell.execute_reply.started":"2023-12-03T00:20:26.686886Z","shell.execute_reply":"2023-12-03T00:20:26.983760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### START CODE HERE ### (≈ 3 lines of code)\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n### END CODE HERE ###\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.986560Z","iopub.execute_input":"2023-12-03T00:20:26.986903Z","iopub.status.idle":"2023-12-03T00:20:26.994652Z","shell.execute_reply.started":"2023-12-03T00:20:26.986873Z","shell.execute_reply":"2023-12-03T00:20:26.993828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px  ∗  num_px  ∗  3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n**","metadata":{}},{"cell_type":"code","source":"# Reshape the training and test examples\n\n### START CODE HERE ### (≈ 2 lines of code)\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n### END CODE HERE ###\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\nprint (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:26.995902Z","iopub.execute_input":"2023-12-03T00:20:26.996403Z","iopub.status.idle":"2023-12-03T00:20:27.013391Z","shell.execute_reply.started":"2023-12-03T00:20:26.996376Z","shell.execute_reply":"2023-12-03T00:20:27.012200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet's standardize our dataset. **","metadata":{}},{"cell_type":"code","source":"train_set_x = train_set_x_flatten/255. # to make train value between 0 , 1\ntest_set_x = test_set_x_flatten/255. # to make test value between 0 , 1","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.015311Z","iopub.execute_input":"2023-12-03T00:20:27.015692Z","iopub.status.idle":"2023-12-03T00:20:27.048651Z","shell.execute_reply.started":"2023-12-03T00:20:27.015662Z","shell.execute_reply":"2023-12-03T00:20:27.047351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common steps for pre-processing a new dataset are:\n\n**Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n\nReshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\n\n\"Standardize\" the data**","metadata":{}},{"cell_type":"markdown","source":"## General Architecture of the learning algorithm\nIt's time to design a simple algorithm to distinguish cat images from non-cat images.\n\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n","metadata":{}},{"cell_type":"markdown","source":"## 4 - Building the parts of our algorithm\nThe main steps for building a Neural Network are:\n\n1. Define the model structure (such as number of input features)\n2. Initialize the model's parameters\n3. Loop:\n      * Calculate current loss (forward propagation)\n      * Calculate current gradient (backward propagation)\n      * Update parameters (gradient descent)\n      \nYou often build 1-3 separately and integrate them into one function we call model().","metadata":{}},{"cell_type":"markdown","source":"## 4.1 - Helper functions","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    ### START CODE HERE ### (≈ 1 line of code)\n    s = 1 / (1 + np.exp(-z))\n    ### END CODE HERE ###\n    \n    return s","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.050524Z","iopub.execute_input":"2023-12-03T00:20:27.051677Z","iopub.status.idle":"2023-12-03T00:20:27.058818Z","shell.execute_reply.started":"2023-12-03T00:20:27.051641Z","shell.execute_reply":"2023-12-03T00:20:27.056989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.063674Z","iopub.execute_input":"2023-12-03T00:20:27.064059Z","iopub.status.idle":"2023-12-03T00:20:27.072422Z","shell.execute_reply.started":"2023-12-03T00:20:27.064029Z","shell.execute_reply":"2023-12-03T00:20:27.071130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 - Initializing parameters","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    w = np.zeros((dim, 1))\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.074666Z","iopub.execute_input":"2023-12-03T00:20:27.075056Z","iopub.status.idle":"2023-12-03T00:20:27.083316Z","shell.execute_reply.started":"2023-12-03T00:20:27.075025Z","shell.execute_reply":"2023-12-03T00:20:27.081690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.084747Z","iopub.execute_input":"2023-12-03T00:20:27.086019Z","iopub.status.idle":"2023-12-03T00:20:27.101670Z","shell.execute_reply.started":"2023-12-03T00:20:27.085975Z","shell.execute_reply":"2023-12-03T00:20:27.100221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 - Forward and Backward propagation","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    # compute activation\n    A = sigmoid(np.dot(w.T,X) + b) \n    # compute cost\n    cost = (-1/m) * np.sum(  (Y *np.log(A)) + ((1-Y) * np.log(1-A)) )\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    db = (1/m) * (np.sum(A-Y))\n    dw = (1/m)*(np.dot(X,np.subtract(A,Y).T))\n    ### END CODE HERE ###\n    assert(db.dtype == float)\n    #assert(dw.shape == w.shape)\n    \n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n\n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.104197Z","iopub.execute_input":"2023-12-03T00:20:27.104943Z","iopub.status.idle":"2023-12-03T00:20:27.117791Z","shell.execute_reply.started":"2023-12-03T00:20:27.104899Z","shell.execute_reply":"2023-12-03T00:20:27.116300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.120643Z","iopub.execute_input":"2023-12-03T00:20:27.121472Z","iopub.status.idle":"2023-12-03T00:20:27.138869Z","shell.execute_reply.started":"2023-12-03T00:20:27.121422Z","shell.execute_reply":"2023-12-03T00:20:27.137212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## d) Optimization","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w, b, X, Y)\n        ### END CODE HERE ###\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        ### START CODE HERE ###\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.140697Z","iopub.execute_input":"2023-12-03T00:20:27.141260Z","iopub.status.idle":"2023-12-03T00:20:27.154694Z","shell.execute_reply.started":"2023-12-03T00:20:27.141230Z","shell.execute_reply":"2023-12-03T00:20:27.153088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:27.156208Z","iopub.execute_input":"2023-12-03T00:20:27.156621Z","iopub.status.idle":"2023-12-03T00:20:27.184335Z","shell.execute_reply.started":"2023-12-03T00:20:27.156588Z","shell.execute_reply":"2023-12-03T00:20:27.182826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5 - Merge all functions into a model","metadata":{}},{"cell_type":"markdown","source":"Implement the model function. Use the following notation: - Y_prediction for your predictions on the test set - Y_prediction_train for your predictions on the train set - w, costs, grads for the outputs of optimize()","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.6, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    #END CODE HERE \n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:33:22.979801Z","iopub.execute_input":"2023-12-03T00:33:22.980339Z","iopub.status.idle":"2023-12-03T00:33:22.993167Z","shell.execute_reply.started":"2023-12-03T00:33:22.980304Z","shell.execute_reply":"2023-12-03T00:33:22.991413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:33:53.168074Z","iopub.execute_input":"2023-12-03T00:33:53.168573Z","iopub.status.idle":"2023-12-03T00:33:55.952080Z","shell.execute_reply.started":"2023-12-03T00:33:53.168538Z","shell.execute_reply":"2023-12-03T00:33:55.950110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:35.684149Z","iopub.status.idle":"2023-12-03T00:20:35.685011Z","shell.execute_reply.started":"2023-12-03T00:20:35.684751Z","shell.execute_reply":"2023-12-03T00:20:35.684777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6 - Further analysis (optional/ungraded exercise)","metadata":{}},{"cell_type":"code","source":"learning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T00:20:35.686997Z","iopub.status.idle":"2023-12-03T00:20:35.687726Z","shell.execute_reply.started":"2023-12-03T00:20:35.687435Z","shell.execute_reply":"2023-12-03T00:20:35.687462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}